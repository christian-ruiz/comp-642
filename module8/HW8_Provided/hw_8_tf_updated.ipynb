{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Tutorial\n",
    "\n",
    "Before doing the coding assignemnt for unit8, you probably need to get yourself familiar with Tensorflow, a open source software library for numerial computation, particulary well suited and fine-tuned for large scale machine learning. The basic principle is you define your computation graph and the tensorflow will take the graph and run it efficiently on optimized c++ code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the tensorflow package\n",
    "\n",
    "if you are using anaconda, you first get into your environment with:\n",
    "\n",
    "```source activate env_name```\n",
    "\n",
    "and then download the tensorflow\n",
    "\n",
    "```conda install -c conda-forge tensorflow```\n",
    "\n",
    "this command will install a cpu version in your machine.\n",
    "\n",
    "if you are not using anaconda, you may want to run this to download tensorflow:\n",
    "\n",
    "```pip install tensorflow```\n",
    "\n",
    "which will install the lastest version tensorflow.\n",
    "\n",
    "For this tutorial we are using python 3.6; tensorflow version 2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-21T03:39:43.087Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating And Running a Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![computation_graph](img/computation_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first goal is to define a computation graph (computation_graph.png) in tensorflow and trigger the computation. Each node in the graph is called operation and each edge represents the flow of the data. The node can either operate on tensors (addition, subtraction, multiplication, etc) or generate a tensor (constant and variable). Each node takes zero or more tensors as inputs and produces a tensor as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-21T03:40:23.257Z"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(3, name = \"x\") \n",
    "y = tf.Variable(4, name = \"y\")\n",
    "two = tf.constant(2)  \n",
    "\n",
    "op1 = tf.multiply(x, x)  \n",
    "op2 = tf.multiply(x, op1)\n",
    "op3 = tf.add(y, two)\n",
    "op4 = tf.add(op2, op3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your operation will be built on a default graph since you didn't specify tf.Graph() which we will talk about later.\n",
    "\n",
    "Once you define your operation, you can start a session and execute your graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-21T03:40:24.276Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess: # starts session, now we can evaluate\n",
    "    x.initializer.run() # Inits vals for x to 3\n",
    "    y.initializer.run()\n",
    "    result = op4.eval() # evaluates op 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You initialize the variable in the graph and and trigger the computation by evaluating the last operation.  Since the op4 is dependent on op2 and op3, it will recursively call evaluation on op2 and op3 until it reaches the leaf node which is the variable and constant defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-21T03:40:28.305Z"
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing the Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-21T03:40:30.153Z"
    }
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-21T03:40:30.540Z"
    }
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create your own graphs and run them in sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-21T03:40:32.948Z"
    }
   },
   "outputs": [],
   "source": [
    "graph1 = tf.Graph()\n",
    "\n",
    "with graph1.as_default():\n",
    "    x = np.random.rand(100).astype(np.float32)\n",
    "    target = x * 0.3 - 0.23\n",
    "    W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "    b = tf.Variable(tf.zeros([1]))\n",
    "    pred = W * x + b\n",
    "    loss = tf.reduce_mean(tf.square(target - pred))\n",
    "    print('num of trainable variables = %d' % len(tf.trainable_variables()))\n",
    "    print('num of global variables = %d' % len(tf.global_variables()))\n",
    "    print('graph1=', graph1)\n",
    "    print('get default graph in current session = ', tf.get_default_graph())\n",
    "    \n",
    "print(\"*\"*100)\n",
    "print('num of trainable variables = %d' % len(tf.trainable_variables()))\n",
    "print('num of global variables = %d' % len(tf.global_variables()))\n",
    "print('global default graph = ' , tf.get_default_graph())\n",
    "print('get default graph in current session = ', tf.get_default_graph())\n",
    "\n",
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    x = np.random.rand(100).astype(np.float32)\n",
    "    target = x * 0.4 - 0.73\n",
    "    W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "    b = tf.Variable(tf.zeros([1]))\n",
    "    pred = W * x + b\n",
    "    loss = tf.reduce_mean(tf.square(target - pred))\n",
    "    print(\"*\"*100)\n",
    "    print('num of trainable variables = %d' % len(tf.trainable_variables()))\n",
    "    print('num of global variables = %d' % len(tf.global_variables()))\n",
    "    print('graph2 = ', graph2)\n",
    "    print('get default graph in current session = ', tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Create Graph with Tensorflow\n",
    "\n",
    "Now it's your turn to practice to define a computation graph in tensorflow (cross_entropy.png). (NOTE : use placeholder to define variable instead of tf.Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cross_entropy](img/cross_entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :: define the cross entorpy computation graph in tensorflow; expect 10-15 lines of code (Requirement : create your own graph with tf.Graph an run your graph; \n",
    "# use placeholder to define variable instead of tf.Variable)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "\n",
    "# TODO :: write down the normal equation, for more detail of the normal equation, you can refer to http://mlwiki.org/index.php/Normal_Equation \n",
    "# hint : you may want to use tf.matrix_inverse, tf.matrix_inverse and tf.matmul\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "# TODO :: implement the same normal equation with numpy\n",
    "# hint : you may want to use np.linalg.inv\n",
    "\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# TODO :: define the linear regression model and fit the training data. the model name should be lin_reg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Gradient Descent\n",
    "\n",
    "Gradient Descent requires scaling the feature vectors first. We could do this using TF, but let's just use Scikit-Learn for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :: define the GradientDescentOptimizer and call minimize on the optimizer, the result should be named as training_op; you can refer to the tf documentation : https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/GradientDescentOptimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :: repeat the same procedure this time use the MomentumOptimizer, you can refer to the tensorflow documentation : https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MomentumOptimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring a model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       \n",
    "learning_rate = 0.01                                                                  \n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            \n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            \n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                     \n",
    "error = y_pred - y                                                                   \n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            \n",
    "training_op = optimizer.minimize(mse)                                                 \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                \n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default the saver also saves the graph structure itself in a second file with the extension .meta. You can use the function tf.train.import_meta_graph() to restore the graph structure. This function loads the graph into the default graph and returns a Saver that can then be used to restore the graph state (i.e., the variable values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "print(tensorboard.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    X = tf.placeholder(tf.float32, name = \"x\")\n",
    "    W1 = tf.placeholder(tf.float32, name = \"W1\")\n",
    "    b1 = tf.placeholder(tf.float32, name = \"b1\")\n",
    "    \n",
    "    a1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    W2 = tf.placeholder(tf.float32, name = \"W2\")\n",
    "    b2 = tf.placeholder(tf.float32, name = \"b2\")\n",
    "    \n",
    "    a2 = tf.nn.relu(tf.matmul(a1, W2) + b2)\n",
    "    \n",
    "    W3 = tf.placeholder(tf.float32, name = \"W3\")\n",
    "    b3 = tf.placeholder(tf.float32, name = \"b3\")\n",
    "    \n",
    "    y_hat = tf.matmul(a2, W3) + b3\n",
    "    \n",
    "# tf.summary.FileWriter(\"logs\", g).close()\n",
    "tf.summary.FileWriter(logdir=\"logs/\", graph=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.print(g, output_stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "path = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = path + \"\\logs\"\n",
    "print(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Activates the tensorboard UI to visualize the graph g\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the commands above are not working, open a new terminal and run the command tensorboard --logdir \"Your logs_path output.\" Then follow the terminal instructions to view the tensorboard visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
